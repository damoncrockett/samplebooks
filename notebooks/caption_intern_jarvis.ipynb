{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec7b855-3d99-4b34-8d12-7dc3ae3ab118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers==4.37.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f216ce1-0d3b-4fb0-a70a-b5a36ee1ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't even know if these are necessary, but most models have required them\n",
    "#!pip install einops accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e1ed9a-74f6-42ec-a64b-e2c4eff0ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f804b48-601b-415e-9f59-7fb8b9990948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30b881a-2757-4f89-81cf-7c599860cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43e0b18-0497-4311-a427-7bac859b4655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_model(model_name):\n",
    "    device_map = {}\n",
    "    world_size = torch.cuda.device_count()\n",
    "    num_layers = {\n",
    "        'InternVL2-1B': 24, 'InternVL2-2B': 24, 'InternVL2-4B': 32, 'InternVL2-8B': 32,\n",
    "        'InternVL2-26B': 48, 'InternVL2-40B': 60, 'InternVL2-Llama3-76B': 80}[model_name]\n",
    "    # Since the first GPU will be used for ViT, treat it as half a GPU.\n",
    "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
    "    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
    "    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
    "    layer_cnt = 0\n",
    "    for i, num_layer in enumerate(num_layers_per_gpu):\n",
    "        for j in range(num_layer):\n",
    "            device_map[f'language_model.model.layers.{layer_cnt}'] = i\n",
    "            layer_cnt += 1\n",
    "    device_map['vision_model'] = 0\n",
    "    device_map['mlp1'] = 0\n",
    "    device_map['language_model.model.tok_embeddings'] = 0\n",
    "    device_map['language_model.model.embed_tokens'] = 0\n",
    "    device_map['language_model.output'] = 0\n",
    "    device_map['language_model.model.norm'] = 0\n",
    "    device_map['language_model.lm_head'] = 0\n",
    "    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n",
    "\n",
    "    return device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009eb840-347f-4d6b-ac50-2976053ff283",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"OpenGVLab/InternVL2-Llama3-76B\"\n",
    "device_map = split_model('InternVL2-Llama3-76B')\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map=device_map).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8006f92-a415-4e9b-8f66-8b56b0ebef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df3ecb5-7cf5-4da1-b4b1-692c8bf5b5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf3c14c-a163-43e7-a2fd-4b32f5fcbeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e5d2a9-a2a7-4360-a0cc-0fb3faaccd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58c2e8e-8df9-46fe-b5a8-752436c92b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2866575d-27fd-4698-80ee-b6b5dc7f9b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    # Check if the input is a URL\n",
    "    if isinstance(image_file, str) and image_file.startswith(\"http\"):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    \n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(img) for img in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    \n",
    "    return pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b03897-04f3-4749-9c11-766936c7830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    " 'https://samplebook.photos/img/4786a.jpg',\n",
    " 'https://samplebook.photos/img/284v.jpg',\n",
    " 'https://samplebook.photos/img/3193c.jpg',\n",
    " 'https://samplebook.photos/img/948d.jpg',\n",
    " 'https://samplebook.photos/img/3191i.jpg',\n",
    " 'https://samplebook.photos/img/4788oo.jpg',\n",
    " 'https://samplebook.photos/img/2086d.jpg',\n",
    " 'https://samplebook.photos/img/4781r.jpg',\n",
    " 'https://samplebook.photos/img/5521n.jpg',\n",
    " 'https://samplebook.photos/img/2073cc.jpg',\n",
    " 'https://samplebook.photos/img/2085b.jpg',\n",
    " 'https://samplebook.photos/img/4782c.jpg',\n",
    " 'https://samplebook.photos/img/4783v.jpg',\n",
    " 'https://samplebook.photos/img/4912gg.jpg',\n",
    " 'https://samplebook.photos/img/304h.jpg',\n",
    " 'https://samplebook.photos/img/2078i.jpg',\n",
    " 'https://samplebook.photos/img/2077q.jpg',\n",
    " 'https://samplebook.photos/img/3192h.jpg',\n",
    " 'https://samplebook.photos/img/904x.jpg',\n",
    " 'https://samplebook.photos/img/4792b.jpg',\n",
    " 'https://samplebook.photos/img/5301p.jpg',\n",
    " 'https://samplebook.photos/img/5207j.jpg',\n",
    " 'https://samplebook.photos/img/1007p.jpg',\n",
    " 'https://samplebook.photos/img/17l.jpg',\n",
    " 'https://samplebook.photos/img/286t.jpg',\n",
    " 'https://samplebook.photos/img/287g.jpg',\n",
    " 'https://samplebook.photos/img/3189j.jpg',\n",
    " 'https://samplebook.photos/img/4911d.jpg',\n",
    " 'https://samplebook.photos/img/4785a.jpg',\n",
    " 'https://samplebook.photos/img/4793a.jpg'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76a8d3a-e6d2-4a80-bfc5-582a3964e644",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'There are two objects in this image. One is a color checker, and the other is a photographic sample book, open to a page containing a photograph. Give a detailed description of the photograph.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afc7646-bcd4-4178-8e59-661fb7d704df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(url):\n",
    "    # set the max number of tiles in `max_num`\n",
    "    pixel_values = load_image(url, max_num=12).to(torch.bfloat16).cuda()\n",
    "    generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "    question = f'<image>\\n{prompt}'\n",
    "    response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceab7b64-95e8-4fce-ab5e-5e4cb50e96e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_intern = {}\n",
    "\n",
    "for url in urls:\n",
    "    response = get_response(url)\n",
    "    print(url,\"\\n\",response,\"\\n\\n\")\n",
    "    responses_intern[url] = {'OpenGVLab/InternVL2-Llama3-76B':response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17f8f17-4600-48ba-93d9-5dc7118ef4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b63bc-0c65-4970-9037-885b62b3f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"responses_intern.pkl\", \"wb\") as f:\n",
    "    pickle.dump(responses_intern, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
