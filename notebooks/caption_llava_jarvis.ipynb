{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2790abfa-2194-4e5e-8689-2ecfebca12d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/LLaVA-VL/LLaVA-NeXT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a420a1-7906-4644-a440-9c7ed0aaf2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install einops accelerate open-clip-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3fb4e5-8217-4819-a850-108f339e233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a805d58-740f-4564-ac50-5b185ebf46c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX\n",
    "from llava.conversation import conv_templates, SeparatorStyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263176ad-4a7c-434f-aeb8-c7c286166d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import copy\n",
    "import torch\n",
    "import sys\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea20be6-dd32-4b78-b576-7623069c4014",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "pretrained = \"lmms-lab/llava-onevision-qwen2-72b-ov\"\n",
    "model_name = \"llava_qwen\"\n",
    "device = \"cuda\"\n",
    "device_map = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa7ab46-32a4-4ea0-9c2f-453a7dce8a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model, image_processor, max_length = load_pretrained_model(pretrained, None, model_name, device_map=device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24506fa-b375-487b-b28e-5e1c77185c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cad756e-dac4-4318-8b30-33ee3952d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    " 'https://samplebook.photos/img/4786a.jpg',\n",
    " 'https://samplebook.photos/img/284v.jpg',\n",
    " 'https://samplebook.photos/img/3193c.jpg',\n",
    " 'https://samplebook.photos/img/948d.jpg',\n",
    " 'https://samplebook.photos/img/3191i.jpg',\n",
    " 'https://samplebook.photos/img/4788oo.jpg',\n",
    " 'https://samplebook.photos/img/2086d.jpg',\n",
    " 'https://samplebook.photos/img/4781r.jpg',\n",
    " 'https://samplebook.photos/img/5521n.jpg',\n",
    " 'https://samplebook.photos/img/2073cc.jpg',\n",
    " 'https://samplebook.photos/img/2085b.jpg',\n",
    " 'https://samplebook.photos/img/4782c.jpg',\n",
    " 'https://samplebook.photos/img/4783v.jpg',\n",
    " 'https://samplebook.photos/img/4912gg.jpg',\n",
    " 'https://samplebook.photos/img/304h.jpg',\n",
    " 'https://samplebook.photos/img/2078i.jpg',\n",
    " 'https://samplebook.photos/img/2077q.jpg',\n",
    " 'https://samplebook.photos/img/3192h.jpg',\n",
    " 'https://samplebook.photos/img/904x.jpg',\n",
    " 'https://samplebook.photos/img/4792b.jpg',\n",
    " 'https://samplebook.photos/img/5301p.jpg',\n",
    " 'https://samplebook.photos/img/5207j.jpg',\n",
    " 'https://samplebook.photos/img/1007p.jpg',\n",
    " 'https://samplebook.photos/img/17l.jpg',\n",
    " 'https://samplebook.photos/img/286t.jpg',\n",
    " 'https://samplebook.photos/img/287g.jpg',\n",
    " 'https://samplebook.photos/img/3189j.jpg',\n",
    " 'https://samplebook.photos/img/4911d.jpg',\n",
    " 'https://samplebook.photos/img/4785a.jpg',\n",
    " 'https://samplebook.photos/img/4793a.jpg'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d0b760-4ed4-4fe2-88f8-01aa536ee2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'There are two objects in this image. One is a color checker, and the other is a photographic sample book, open to a page containing a photograph. Give a detailed description of the photograph.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecacc1a5-790d-43b0-974e-eab07e627c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(url):\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "    image_tensor = process_images([image], image_processor, model.config)\n",
    "    image_tensor = [_image.to(dtype=torch.float16, device=device) for _image in image_tensor]\n",
    "    \n",
    "    conv_template = \"qwen_1_5\"  # Make sure you use correct chat template for different models\n",
    "    question = DEFAULT_IMAGE_TOKEN + f'\\n{prompt}'\n",
    "    conv = copy.deepcopy(conv_templates[conv_template])\n",
    "    conv.append_message(conv.roles[0], question)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt_question = conv.get_prompt()\n",
    "    \n",
    "    input_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(device)\n",
    "    image_sizes = [image.size]\n",
    "    \n",
    "    cont = model.generate(\n",
    "        input_ids,\n",
    "        images=image_tensor,\n",
    "        image_sizes=image_sizes,\n",
    "        do_sample=False,\n",
    "        temperature=0,\n",
    "        max_new_tokens=1024,\n",
    "    )\n",
    "    text_outputs = tokenizer.batch_decode(cont, skip_special_tokens=True)\n",
    "\n",
    "    return text_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0148ec-87c3-45fb-9996-26b66ce4973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_llava = {}\n",
    "\n",
    "for url in urls:\n",
    "    response = get_response(url)\n",
    "    print(url,\"\\n\",response,\"\\n\\n\")\n",
    "    responses_llava[url] = {'lmms-lab/llava-onevision-qwen2-72b-ov':response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c300cced-db0b-4478-8c63-e99ad8c238ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94120a74-e013-45d2-b1db-5c7bdd4322b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"responses_llava.pkl\", \"wb\") as f:\n",
    "    pickle.dump(responses_llava, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
