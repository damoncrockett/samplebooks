{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2790abfa-2194-4e5e-8689-2ecfebca12d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/LLaVA-VL/LLaVA-NeXT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55a420a1-7906-4644-a440-9c7ed0aaf2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install einops accelerate open-clip-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a3fb4e5-8217-4819-a850-108f339e233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a805d58-740f-4564-ac50-5b185ebf46c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install pyav to use video processing functions.\n"
     ]
    }
   ],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX\n",
    "from llava.conversation import conv_templates, SeparatorStyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "263176ad-4a7c-434f-aeb8-c7c286166d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import copy\n",
    "import torch\n",
    "import sys\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ea20be6-dd32-4b78-b576-7623069c4014",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "pretrained = \"lmms-lab/llava-onevision-qwen2-72b-ov\"\n",
    "model_name = \"llava_qwen\"\n",
    "device = \"cuda\"\n",
    "device_map = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baa7ab46-32a4-4ea0-9c2f-453a7dce8a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LLaVA model: lmms-lab/llava-onevision-qwen2-72b-ov\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ab4d2515734190be05337223c274d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77bb58e3b1a24b01af9af2c98ac6e26c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600df090b9f44d2699c006ad44bcc156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de977303b19a48a098303739d811544d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43319dbb710248ba821938ae3414e752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/101 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77778e15b6ef40619306b78ba4fdffb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/367 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02e7912d8f048a3928589c211e44a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/3.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d2660e7015465e977bdd8e950607ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/132k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41acb5d821d413bbfeac10670820d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c040193e49174856aca048f20cfa19cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00031.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0419efc5b6748718519b12919bd6323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00031.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075246d4648e4247b70b1115ddf92adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00031.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672dc2ee9fab4dafb6b84edd3dd57b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00031.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a92fe98b6a64e79a0d14bd130031785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00031.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544ca0d689f74dd0b1bee08a7d0a57ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00031.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7eda4d997944a6086041545183d3815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00031.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e2c50ceca6445f684a7e24225107c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00031.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2086c33e0764ec3b6698bda86aae840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00031.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0cf5d5e2c34210adc3824e0b93391f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00031.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f428411553541cbacaaf6623a213c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00031.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba22538e9e72416eb22d656a14aeed75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00031.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a315ff0679194a92b6b50cf82e277997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00031.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef80561434c4e56b301df483d0ae2a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00031.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20e3c4885e642fc909733ac433a057f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00015-of-00031.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b4fc39a3ad48c68b5df6c0afa4c0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00016-of-00031.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b13e71dc04443f0a077c224930d0019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00017-of-00031.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a270e4754ec34a54842bb5718c15135f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00018-of-00031.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a813203a054fcdb86eb5764075d816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00019-of-00031.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8ce595674d430986c7b2a05a2b92ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00020-of-00031.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b5b583f9584443a50abe0599dea08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00021-of-00031.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa314ebf1e44da1b6f8111d0bc8354c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00022-of-00031.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b32f16af1b4d22b3564c1f7864a684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00023-of-00031.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad700715bf534d93813bd761d5545682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00024-of-00031.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40fc39f39ee477e9907bb2561145991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00025-of-00031.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796cd18735b94ab2b4fa4ef32c44cc2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00026-of-00031.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c7f528a840472281d34ea3dd75fdc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00027-of-00031.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9d5743b3bb4906876a509da470a36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00028-of-00031.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fe196c04584aee8490ccf592dd6499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00029-of-00031.safetensors:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66bf661216144ff8b4e58421d573fa56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00030-of-00031.safetensors:   0%|          | 0.00/4.16G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8d42fb492c4b66ab63ac32b6f64b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00031-of-00031.safetensors:   0%|          | 0.00/2.49G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vision tower: google/siglip-so400m-patch14-384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b3a6cc17e74882abbb0e4f589654d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/576 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd4d932cd6c4a6c8c07b055a4041172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2baaebaa096f4139ab92f5a9b1ecc614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d44e3dc08be4c9a965f42e7d48abd2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/248 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Class: LlavaQwenForCausalLM\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model, image_processor, max_length = load_pretrained_model(pretrained, None, model_name, device_map=device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a24506fa-b375-487b-b28e-5e1c77185c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlavaQwenForCausalLM(\n",
       "  (model): LlavaQwenModel(\n",
       "    (embed_tokens): Embedding(151647, 8192)\n",
       "    (layers): ModuleList(\n",
       "      (0-79): 80 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2FlashAttention2(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=True)\n",
       "          (k_proj): Linear(in_features=8192, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=8192, out_features=1024, bias=True)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=29568, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=29568, bias=False)\n",
       "          (down_proj): Linear(in_features=29568, out_features=8192, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((8192,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((8192,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((8192,), eps=1e-06)\n",
       "    (vision_tower): SigLipVisionTower(\n",
       "      (vision_tower): SigLipVisionModel(\n",
       "        (vision_model): SigLipVisionTransformer(\n",
       "          (embeddings): SigLipVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "            (position_embedding): Embedding(729, 1152)\n",
       "          )\n",
       "          (encoder): SigLipEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-25): 26 x SigLipEncoderLayer(\n",
       "                (self_attn): SigLipAttention(\n",
       "                  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): SigLipMLP(\n",
       "                  (activation_fn): PytorchGELUTanh()\n",
       "                  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (head): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (vision_resampler): IdentityMap()\n",
       "    (mm_projector): Sequential(\n",
       "      (0): Linear(in_features=1152, out_features=8192, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=8192, out_features=8192, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=8192, out_features=151647, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cad756e-dac4-4318-8b30-33ee3952d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    " 'https://samplebook.photos/img/4786a.jpg',\n",
    " 'https://samplebook.photos/img/284v.jpg',\n",
    " 'https://samplebook.photos/img/3193c.jpg',\n",
    " 'https://samplebook.photos/img/948d.jpg',\n",
    " 'https://samplebook.photos/img/3191i.jpg',\n",
    " 'https://samplebook.photos/img/4788oo.jpg',\n",
    " 'https://samplebook.photos/img/2086d.jpg',\n",
    " 'https://samplebook.photos/img/4781r.jpg',\n",
    " 'https://samplebook.photos/img/5521n.jpg',\n",
    " 'https://samplebook.photos/img/2073cc.jpg',\n",
    " 'https://samplebook.photos/img/2085b.jpg',\n",
    " 'https://samplebook.photos/img/4782c.jpg',\n",
    " 'https://samplebook.photos/img/4783v.jpg',\n",
    " 'https://samplebook.photos/img/4912gg.jpg',\n",
    " 'https://samplebook.photos/img/304h.jpg',\n",
    " 'https://samplebook.photos/img/2078i.jpg',\n",
    " 'https://samplebook.photos/img/2077q.jpg',\n",
    " 'https://samplebook.photos/img/3192h.jpg',\n",
    " 'https://samplebook.photos/img/904x.jpg',\n",
    " 'https://samplebook.photos/img/4792b.jpg',\n",
    " 'https://samplebook.photos/img/5301p.jpg',\n",
    " 'https://samplebook.photos/img/5207j.jpg',\n",
    " 'https://samplebook.photos/img/1007p.jpg',\n",
    " 'https://samplebook.photos/img/17l.jpg',\n",
    " 'https://samplebook.photos/img/286t.jpg',\n",
    " 'https://samplebook.photos/img/287g.jpg',\n",
    " 'https://samplebook.photos/img/3189j.jpg',\n",
    " 'https://samplebook.photos/img/4911d.jpg',\n",
    " 'https://samplebook.photos/img/4785a.jpg',\n",
    " 'https://samplebook.photos/img/4793a.jpg'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4d0b760-4ed4-4fe2-88f8-01aa536ee2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'There are two objects in this image. One is a color checker, and the other is a photographic sample book, open to a page containing a photograph. Give a detailed description of the photograph.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecacc1a5-790d-43b0-974e-eab07e627c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(url):\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "    image_tensor = process_images([image], image_processor, model.config)\n",
    "    image_tensor = [_image.to(dtype=torch.float16, device=device) for _image in image_tensor]\n",
    "    \n",
    "    conv_template = \"qwen_1_5\"  # Make sure you use correct chat template for different models\n",
    "    question = DEFAULT_IMAGE_TOKEN + f'\\n{prompt}'\n",
    "    conv = copy.deepcopy(conv_templates[conv_template])\n",
    "    conv.append_message(conv.roles[0], question)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt_question = conv.get_prompt()\n",
    "    \n",
    "    input_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(device)\n",
    "    image_sizes = [image.size]\n",
    "    \n",
    "    cont = model.generate(\n",
    "        input_ids,\n",
    "        images=image_tensor,\n",
    "        image_sizes=image_sizes,\n",
    "        do_sample=False,\n",
    "        temperature=0,\n",
    "        max_new_tokens=1024,\n",
    "    )\n",
    "    text_outputs = tokenizer.batch_decode(cont, skip_special_tokens=True)\n",
    "\n",
    "    return text_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa0148ec-87c3-45fb-9996-26b66ce4973b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://samplebook.photos/img/4786a.jpg \n",
      " ['The photograph in the sample book depicts a rugged landscape, possibly a rocky coastline or mountainous terrain. The image is in black and white, emphasizing the textures and contrasts of the natural environment. There are no visible human figures or man-made structures, suggesting a focus on the raw beauty of nature. The photograph appears to be taken during the day, as indicated by the bright lighting and clear visibility of details.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/284v.jpg \n",
      " ['The photograph in the sample book depicts a large industrial structure, possibly an electrical substation or power plant. The image is in black and white, emphasizing the contrast between the metallic components and the surrounding environment. The structure features multiple cylindrical tanks, tall metal towers with intricate latticework, and numerous cables and wires connecting various parts of the facility. The ground appears to be covered with gravel or small stones, and there are no visible people or moving objects, giving the scene a static and somewhat desolate appearance. The photograph has a vintage feel, suggesting it may have been taken several decades ago.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/3193c.jpg \n",
      " ['The photograph in the sample book depicts a well-furnished room with a chandelier hanging from the ceiling. The room has several pieces of furniture, including a sofa, chairs, and a table. There are also framed pictures on the walls and a potted plant in one corner of the room. The photograph is in black and white, giving it a classic and timeless feel.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/948d.jpg \n",
      " ['The photograph in the sample book depicts a serene landscape scene. It features a dense collection of trees with varying shades of foliage, suggesting a forest or woodland area. The trees are tall and appear to be mature, with thick trunks and branches that spread outwards. There is a clear path or road visible in the foreground, leading into the depth of the forest. The photograph has a vintage look, possibly indicating it was taken some time ago. The image is in black and white, which adds to its timeless quality. The photograph is mounted on a page with text at the bottom, which appears to be a description or caption related to the image.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/3191i.jpg \n",
      " ['The photograph in the sample book depicts a historical urban scene, likely from a European city. It features a grand staircase leading up to an imposing building with classical architecture, including columns and ornate details. The staircase is flanked by statues, adding to the grandeur of the scene. There are people visible at the bottom of the stairs, suggesting that this is a public space. The photograph has a vintage feel, possibly indicating it was taken in the early 20th century. The image is in black and white, which adds to its timeless quality.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/4788oo.jpg \n",
      " ['The photograph in the sample book is a black and white image of an elderly man with a prominent nose and a mustache. He is wearing a dark-colored shirt and appears to be smoking a pipe, as there is smoke visible around his mouth. The man has a serious expression on his face and is looking directly at the camera. The background of the photograph is plain and does not contain any distinguishable features. There is text at the bottom of the photograph that reads \"Dufour\" and \"A. G.\", which could possibly be the name of the photographer or the subject of the photograph.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/2086d.jpg \n",
      " ['The photograph in the sample book is a black and white image featuring a person who appears to be lying down, with their head resting on another individual\\'s lap. The person whose head is resting has their eyes closed and seems to be in a state of rest or possibly unconsciousness. The individual whose lap is being used as a pillow is seated and looking down at the person with their head resting on their lap. The background is indistinct, but it appears to be an outdoor setting with some foliage visible. The photograph has a textured appearance, suggesting it may have been taken using a specific photographic paper or technique. There is text at the bottom of the photograph that reads \"KODAK EKTALURE Paper X,\" indicating the type of photographic paper used for the print.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/4781r.jpg \n",
      " ['The photograph in the sample book features a man sitting with his arms crossed, wearing a white shirt. He is smiling and looking slightly to his right. The background appears to be a plain, light-colored wall. The image has a vintage or retro feel to it, possibly due to the color tone and style of the clothing.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/5521n.jpg \n",
      " ['The photograph in the sample book shows a stack of black and white belts, with each belt having a different pattern. The belts are arranged vertically, with the top belt being the thickest and the bottom belt being the thinnest. The background is dark, which makes the belts stand out prominently. The image appears to be taken from a slight angle, giving a three-dimensional effect to the stack of belts.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/2073cc.jpg \n",
      " [\"The photograph in the sample book is a black and white aerial view of a city. The image shows a dense arrangement of buildings, streets, and other urban structures. The perspective is from above, providing a bird's-eye view of the cityscape. The photograph appears to be taken during the day, as there are no visible shadows or lighting that would suggest a different time of day. The image is clear and detailed, allowing for individual buildings and streets to be distinguished. There are no visible texts or markings on the photograph itself.\"] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/2085b.jpg \n",
      " ['The photograph in the sample book depicts a serene beach scene. A person, possibly a woman based on the attire and hairstyle, is standing on the sandy shore, looking out towards the water. The individual is wearing a light-colored dress or outfit that contrasts with the darker tones of the beach. The beach itself appears to be rocky, with small stones scattered across the foreground. The water is calm, reflecting the light from the sky, which suggests it might be either early morning or late afternoon. The overall mood of the photograph is peaceful and contemplative.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/4782c.jpg \n",
      " ['The photograph in the sample book depicts a scene on a boat. There are two individuals visible, one seated and the other standing. The person seated is wearing a long garment that drapes over their legs, while the standing individual appears to be wearing a head covering. The boat has a large sail that is partially furled, with ropes and rigging visible. The background shows a body of water, suggesting that the boat is in motion. The photograph is in black and white, giving it a vintage or historical feel.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/4783v.jpg \n",
      " ['The photograph in the sample book features a small elephant figurine and a vase with intricate designs. The elephant is positioned in front of the vase, both resting on a reflective surface that creates subtle reflections beneath them. The background is dark, which accentuates the details and textures of the objects. The lighting appears to be coming from above, casting soft shadows behind the objects, adding depth to the image.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/4912gg.jpg \n",
      " ['The photograph in the sample book depicts a serene landscape featuring a river flowing through a lush, green valley. The river is surrounded by dense foliage and trees, creating a tranquil and picturesque scene. The photograph appears to be taken during the day, with natural light illuminating the scene and casting soft shadows on the landscape. The colors in the photograph are vibrant and rich, with the greens of the foliage contrasting beautifully against the blue of the sky and the brown of the riverbank. Overall, the photograph captures the beauty and tranquility of nature in a stunning and detailed way.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/304h.jpg \n",
      " ['The photograph in the sample book shows a person operating an enlarging computer, which is a device used for making correctly exposed enlargements. The image captures the individual adjusting the settings on the machine, with various dials and knobs visible. The background includes a darkroom environment, suggesting that this is a professional or serious amateur setting for developing photographs.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/2078i.jpg \n",
      " ['The photograph in the sample book shows a vintage Ilford camera, model A.2, placed on a glossy surface. The camera is positioned at an angle, with its lens facing towards the top right corner of the image. The background is dark, which contrasts with the light-colored camera and highlights its details. The photograph has a classic, timeless feel to it, reminiscent of mid-20th-century photography.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/2077q.jpg \n",
      " ['The photograph in the sample book depicts a mountainous landscape with a prominent peak and a valley below. The image is in black and white, capturing the contrast between the dark shadows of the mountains and the lighter tones of the sky. The photograph appears to be taken from a distance, providing a wide view of the scene. The composition emphasizes the grandeur and majesty of the natural environment.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/3192h.jpg \n",
      " ['The photograph in the sample book depicts a serene landscape viewed through an archway. The archway frames the scene, drawing attention to the tall, slender cypress trees that stand prominently in the foreground. Beyond the cypress trees, rolling hills stretch into the distance under a clear sky. The image is in black and white, emphasizing the contrast between the dark archway and the lighter tones of the landscape. The composition creates a sense of depth and perspective, inviting the viewer to imagine stepping through the archway and into the tranquil scene beyond.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/904x.jpg \n",
      " ['The photograph in the sample book depicts a serene winter scene. It shows a snow-covered landscape with trees and bushes, some of which are bare while others have snow-laden branches. The ground is blanketed in white snow, and there appears to be a path or clearing in the center of the image. The photograph has a vintage feel, possibly due to its age or the photographic technique used. There is text at the bottom of the photograph that reads \"CARBON VELVET\" and \"No. 1 AUTOMATIC KODAK JUNIOR,\" indicating that it may have been taken with a Kodak Junior camera using carbon velvet paper.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/4792b.jpg \n",
      " ['The photograph in the sample book appears to be a black and white image of a vintage typewriter. The typewriter is positioned at an angle, with the keys facing towards the viewer. The image has a high contrast, with the dark tones of the typewriter standing out against the lighter background. The photograph has a classic and nostalgic feel, evoking a sense of history and craftsmanship.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/5301p.jpg \n",
      " ['The photograph in the sample book features a striking black and white image of a person wearing a dark, flowing garment that drapes elegantly around their body. The individual is positioned in profile, with their face partially obscured by the fabric, adding an air of mystery to the composition. The background is minimalistic, with a plain surface that contrasts sharply with the intricate textures of the garment. The lighting is soft yet dramatic, highlighting the contours of the fabric and creating a sense of depth and dimensionality. The overall effect is one of sophistication and artistic expression.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/5207j.jpg \n",
      " ['The photograph in the sample book depicts a serene scene of two dogs lying on a textured surface, possibly a rug or blanket. The dog on the left appears to be a medium-sized breed with a light-colored coat, while the dog on the right has a darker coat and is slightly larger. Both dogs are resting comfortably, with their heads on the ground, suggesting a moment of relaxation or rest. The background is blurred, drawing focus to the dogs as the main subjects of the image. The overall tone of the photograph is warm and cozy, capturing a peaceful moment between the two animals.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/1007p.jpg \n",
      " ['The photograph in the sample book shows a young child standing and smiling. The child is wearing a short-sleeved shirt, shorts, and oversized shoes that appear to be too big for their feet. The background of the photograph is plain and light-colored, providing a clear contrast to the subject. The image has a vintage or historical appearance, suggesting it may have been taken some time ago. There is text at the bottom of the photograph, which appears to be a watermark or signature, but the specific details are not legible in this description.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/17l.jpg \n",
      " ['The photograph in the sample book features a woman standing in front of a brick wall. She is wearing a white dress and holding a tennis racket over her shoulder. The woman has blonde hair and is looking directly at the camera with a slight smile. The lighting in the photograph appears to be natural, with shadows cast on the wall behind her. The overall tone of the photograph is vintage, suggesting it may have been taken several decades ago.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/286t.jpg \n",
      " ['The photograph in the sample book shows two women standing outdoors. They are dressed in vintage clothing, with one woman wearing a full-length dress and the other wearing a shorter dress with a skirt. The setting appears to be a garden or park, with trees and shrubbery in the background. The image is in black and white, and there is text at the bottom of the page that reads \"ANSCO CONVIRA\" and \"Velvet DW Grades 2, 3, 4.\"'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/287g.jpg \n",
      " [\"The photograph in the sample book features two individuals, likely women, sitting on a ledge or wall. One is leaning against the other, who has her arm around the first individual's shoulder. They are dressed in casual attire, with one wearing a jacket and the other in a long-sleeved shirt. The setting appears to be an urban environment, possibly a street or alley, with a window and a door visible in the background. The image has a candid, relaxed feel, capturing a moment of companionship between the two subjects.\"] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/3189j.jpg \n",
      " ['The photograph in the sample book depicts a serene rural landscape. A dirt road stretches into the distance, flanked by fields on either side. In the background, rolling hills or low mountains can be seen under a clear sky. The image has a vintage feel, possibly due to its sepia tone and the style of the photograph, which suggests it might be from an earlier era. The photograph is framed within a gold border, adding a touch of elegance to the presentation.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/4911d.jpg \n",
      " ['The photograph in the sample book depicts a serene rural scene. In the foreground, there is a dirt road that leads to a small cluster of buildings, possibly a farm or a village. The buildings are surrounded by lush greenery, with trees and bushes providing shade and shelter. There are also a few animals visible, including what appears to be a cow and some chickens. The sky is clear, suggesting a calm and peaceful day. Overall, the photograph captures the simplicity and beauty of rural life.'] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/4785a.jpg \n",
      " [\"The photograph in the sample book appears to be an old, faded image of a person. The person's face is not clearly visible due to the age and condition of the photograph. The background of the photograph seems to be a landscape or outdoor scene, but it is difficult to make out specific details. The photograph has a sepia tone, indicating that it may be quite old. There are also some stains or marks on the photograph, which further contribute to its aged appearance.\"] \n",
      "\n",
      "\n",
      "https://samplebook.photos/img/4793a.jpg \n",
      " ['The photograph in the sample book shows a person, likely a woman, wearing a striped garment with a pattern that includes diagonal lines. The individual is seated and appears to be engaged in an activity involving their hands, possibly holding or manipulating an object that is not clearly visible. The background of the photograph is indistinct, with no discernible features or objects. The image has a vintage appearance, suggesting it may be from an earlier time period.'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "responses_llava = {}\n",
    "\n",
    "for url in urls:\n",
    "    response = get_response(url)\n",
    "    print(url,\"\\n\",response,\"\\n\\n\")\n",
    "    responses_llava[url] = {'lmms-lab/llava-onevision-qwen2-72b-ov':response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c300cced-db0b-4478-8c63-e99ad8c238ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94120a74-e013-45d2-b1db-5c7bdd4322b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"responses_llava.pkl\", \"wb\") as f:\n",
    "    pickle.dump(responses_llava, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
